\section{Introduction}

\textbf{Requirements for MPC}

1. A model of the system

2. A state estimator

3. Define the optimal control problem

4. Set up the optimization problem

5. Get the optimal control sequence (solve the optimization problem)

6. Verify that the closed-loop system performs as desired

%TODO: models of system,table, (non)linear

\subsection{Linearization}

\textbf{Exact solution}
$x(t) = e^{A^c(t-t_0)}x_0 +
	\int_{t_0}^{t}e^{A^c(t-\tau)}B^c u(\tau)d\tau$

\textbf{Problem} Most physical systems are nonlinear
% but linear systems are much better understood

\textbf{Idea} use First Order Taylor expansion
$f(\bar{x}) + \left. \frac{\partial f}{\partial x^\top} \right
	\rvert_{\bar{x}} (x-\bar{x})$

\[\begin{aligned}
		\dot{x_s} & =g(x_s,u_s) = 0
		          & \Delta \dot{x}  =\dot{x} -\dot{x_s}
		          & = A^c\Delta x + B^c\Delta u
		\\
		y_s       & = h(x_s,u_s)
		          & \Delta y        = y - y_s
		          & = C\Delta x + D\Delta u
	\end{aligned} \]

$A^c= \left.\frac{\partial g}{\partial x^T}\right|_{\substack{x_s \\u_s}} $
$B^c= \left.\frac{\partial g}{\partial u^T}\right|_{\substack{x_s \\u_s}} $
$C= \left.\frac{\partial h}{\partial x^T}\right|_{\substack{x_s \\u_s}} $
$D= \left.\frac{\partial h}{\partial u^T}\right|_{\substack{x_s \\u_s}} $

%TODO: Example linearization

\subsection{Discretization}

For general nonlinear systems only approximate discretization methods
exist, such as Euler, quality depends on sampling time

\begin{minipage}[c]{0.5\linewidth}
	\textbf{Approximation}\\
	\[
		\dot{x}^c \approx \frac{x^c(t + T_s)-x^c(t)}{T_s}
	\]
\end{minipage}
\begin{minipage}[c]{0.45\linewidth}
	\center\textbf{Notation}
	$x(k):= x^c(t_0+kT_s)$\\
	$u(k):= u^c(t_0+kT_s)$\\
\end{minipage}
\textbf{Exact Discretization of Linear Time-Invariant Models}
\[\begin{aligned}
		x(t_{k+1}) & =
		\underbrace{e^{A^c T_s}}
		_{=A} x(t_k) +
		\underbrace{\textstyle\int_{0}^{T_s}
			e^{A^c(T_s - \tau)}B^c d\tau}
		_{B=(A^c)^{-1}(A-\mathbb{I})B^c} u(t_k)
		\\
		x(k+N)     & = A^N x(k) + \textstyle\sum_{i=0}^{N-1}
		A^i B u(k+N-1-i)
	\end{aligned} \]

\subsection{Analysis of LTI Discrete-Time Systems}

%NOTE: Coordinate  Transform


\textbf{Controllabe}
$
	\text{if rank}(\mathcal{C}) = n
	,\;
	\mathcal{C} =
	\begin{bmatrix}
		B & \cdots & A^{n-1} B
	\end{bmatrix}
$

$\forall(x(0),x^*)\exists$ finite time $N$
with inputs $\mathcal{U}$, s.t. $x(N)=x^*$

\textbf{Stabilizable} iff all uncontrollable modes stable

\textbf{Observable}
$
	\text{if rank}(\mathcal{O}) = n
	,\;
	\begin{bmatrix}
		C^\top & \cdots & (CA^{n-1})^\top
	\end{bmatrix}^\top
$

$\forall x(0)\exists$ finite time $N$, s.t. the measurements

$y(0), \dots, y(N-1)$
uniquely distinguish initial state $x(0)$

\textbf{Detectablitiy} iff all unobservable modes stable

\subsection{Lyapunov}
%WARN: Lyapunov, direct, indirect

Stability is a property of an
\textbf{equilibrium point} $\bar{\mathbf{x}}$
of a system

\begin{definition}[Lyapunov Stability]
	$\bar{\mathbf{x}}$ is \textbf{Lyapunov stable} if:

	$\forall\ \epsilon>0\ \exists\ \delta(\epsilon)$ s.t.
	$\lvert\lvert x(0) - \bar{x} \rvert\rvert < \delta(\epsilon) \to
		\lvert\lvert x(k) - \bar{x} \rvert\rvert < \epsilon$
\end{definition}

\begin{definition}[Globally asymptotic stability]
	If $\bar{\mathbf{x}}$ is Lyapunov stable and attractive, i.e.,
	$\lim_{k\to\infty} ||x(k)-\bar{x}||=0,\ \forall x(0)$
	then $\bar{\mathbf{x}}$ is \textbf{globally asymptotic stable}.
\end{definition}

\begin{sstFrame}[RoyalBlue!50]
	\begin{definition}[Global Lyapunov function]
		For  $\bar{\mathbf{x}}=0$,
		function $V:\mathbb{R}^n\to \mathbb{R}$ is called
		\textbf{Lyapunov function} if it is continuous at the origin,
		finite $\forall\ x\in \mathbb{R}^{n}$,
		\[
			||x||\to\infty\Rightarrow V(x)\to\infty
		\]
		\[
			V(x)>0\ \forall\ x\in\mathbb{R}^n \setminus\{0\}\quad V(0)=0
		\]
		\[
			V(g(x)) - V(x) \leq -\alpha(x) \quad \forall x \in \mathbb{R}^n
		\]

		where $\alpha:\mathbb{R}^n\to \mathbb{R}$
		continuous positive definite
	\end{definition}
\end{sstFrame}

\begin{sstTitleBox}[Plum]{\center\textbf{\large
			Lyapunov Theorem
		}}
	\begin{theorem}
		%TODO: system (1)
		If a system admits a Lyapunov function $V(x)$,
		then $\bar{\mathbf{x}} = 0$ is
		\textbf{globally asymptotically stable}.
	\end{theorem}
	\begin{theorem}[Lyapunov indirect method]
		For linearization of system around $\bar{\mathbf{x}}=0$
		and resulting matrix
		$A=\left.\frac{\partial g}{\partial x^T}\right|_{x=0}$
		$|\lambda_i|<1$ %HACK: finish Lyapunov indirect
	\end{theorem}
\end{sstTitleBox}
\subsection{Optimal Control}

Unconstrained Finite Horizon Control Problem

$$\begin{aligned}
		J^\star(x(0)) :=
		\min_U & x_N^\top P x_N +
		\textstyle\sum_{i=0}^{N-1}
		x_i^\top Q x_i + u_i^\top R u_i
		\\
		\text{subject to  }
		       & x_{i+1}                = Ax_i+Bu_i
		\quad i  = 0,\dots,N-1                      \\
		       & x_0                    = x(0)
	\end{aligned}$$

\subsection{Batch Approach}

$$ \begin{bmatrix}
		x_0    \\
		x_1    \\
		\vdots \\
		x_N
	\end{bmatrix}
	=
	\begin{bmatrix}
		\mathbb{I} \\
		A          \\
		\vdots     \\
		A^N
	\end{bmatrix}
	x(0) +
	\begin{bmatrix}
		0        & \cdots & 0 \\
		B        & 0      & 0 \\
		AB       & B      & 0 \\
		\vdots   & \ddots & 0 \\
		A^{N-1}B & \cdots & B
	\end{bmatrix}
	\begin{bmatrix}
		u_0    \\
		u_1    \\
		\vdots \\
		u_{N-1}
	\end{bmatrix} $$

\textbf{Cost}

$\overline{Q} := \mathop{\mathrm{blockdiag}}(Q,\dots, Q,P)$

$\overline{R} := \mathop{\mathrm{blockdiag}}(R,\dots, R)$

\textbf{Optimal Input}
$$ U^\star(x(0)) =
	- \bigl(
	\underbrace{
			(\mathcal{S}^u)^\top \overline{Q} \mathcal{S}^u + \overline{R}
		}_{H\text{(Hessian)}^{-1}}
	\bigr)
	\underbrace{
		(\mathcal{S}^u)^\top \overline{Q}\mathcal{S}^x
	}_ {F^\top}
	x(0)
$$

%HACK: Batch Approach

\textbf{Optimal Cost}

\scalebox{0.97}[1]{$
		\scriptstyle
		J^\star(x(0)) = x(0)^\top (
		\mathcal{S}_x^\top \overline{Q} \mathcal{S}_x
		- \mathcal{S}_x^\top \overline{Q} \mathcal{S}_u
		(\mathcal{S}_u^\top \overline{Q} \mathcal{S}_u
		+ \overline{R})^{-1}
		\mathcal{S}_u^\top \overline{Q} \mathcal{S}_x
		)x(0)$}

\subsection{Recursive Approach}

%HACK: Recursive Approach

%HACK: comparison batch,recursivje


\begin{sstTitleBox}[ForestGreen]{\textbf{\large
			\subsection{Infinite Horizon LQR}
		}
		% small text
	}

	\[ \begin{aligned}
			J_\infty^\star(x(k))  & = \min \sum_{i=0}^\infty
			%HACK: LQR min ?
			x_i^\top Q x_i + u_i^\top R u_i                  \\
			\mathrm{subj.\ to }\  & x_{i+1} = Ax_i + Bu_i    \\
			                      & x_0 = x(k)
		\end{aligned} \]

	% \begin{centering}
	% 	\begin{sstFrame}[ForestGreen]
	% 		\vspace{-1.5mm}
	% 		\color{white}
	% 		\[ \begin{aligned}
	%        % formula
	% 			\end{aligned} \]
	% 		\vspace{-2.5mm}
	% 	\end{sstFrame}
	% 	\[ \begin{aligned}
	%      %formula
	% 		\end{aligned} \]
	% \end{centering}

\end{sstTitleBox}

\section{Optimization}

%TODO: epigraph

A mathematical optimization problem is generally formulated as:

\begin{equation}
	\begin{aligned}
		\min_{x\in\operatorname{dom}(f)}          & f(x)                      \\
		\text{s.t. }                       g_i(x) & \le 0,\quad i = 1,\dots,m \\
		h_i(x)                                    & = 0,\quad i = 1,\dots,p   \\
	\end{aligned}
	\label{eq:optimization}
\end{equation}

- $x = (x_1,...,x_n) \in \mathbb{R}^{n}$ decision variable

- $f: \operatorname{dom}(f)\to\mathbb{R}$ objectivce function

- $\mathcal{X} = \{x \in \mathbb{R}^{n}: g(\xi)\le0,\ h(\xi)=0\}$ fesabile set

%TODO: further definitions

\subsection{Convex Sets}

\begin{definition}[Convex Set]
	A set $\mathcal{C}$ is convex if and only if
	$$\theta x + (1-\theta)y \in \mathcal{C}
		\forall\ x,y \in \mathcal{C},\quad
		\forall\ \theta \in [0,1]$$
\end{definition}

(hyperplane $\parallel$ half-space)
$\{x \in \mathbb{R}^n \mid a\T x(=\parallel\le)b\}$

polyhedra $\{x\in\mathbb{R}^n\mid A^{q\times n}x\preceq b^{q\times1},C^{r\times n}x=d^{r\times1}\}$
%TODO: check

polytope

%WARN: Ellipsoid

%TODO: Norm Ball

\textbf{Intersection}
$\mathcal{C}_1, \mathcal{C}_2$ cv
$\Rightarrow \mathcal{C}_1 \cap \mathcal{C}_2$ convex \textbf{(cv)}

\textbf{Image under affine map}
$\mathcal{C} \subseteq  \mathbb{R}^{n}$ cv
$\Rightarrow \{Ax+b \mid x \in \mathcal{C} \}$ cv

\textbf{Inverse IoaM}
$\mathcal{C} \subseteq  \mathbb{R}^{m}$ cv
$\Rightarrow \{x\in\mathbb{R}^{n} \mid  Ax+b\in\mathcal{C}\}$ cv

\subsection{Convex Functions}

%TODO: Convex Function Definition

\textbf{Check Convexity} $f$ is convex if it is
composition of simple convex function
with convexity preserving operations
or if


$f: \mathbb{R}^n \rightarrow \mathbb{R}$ twice differentiable,
$\partial^2f/\partial x^2 \succeq 0\ \forall\ x \in \mathbb{R}^{n}$

$g: \mathbb{R} \rightarrow \mathbb{R}$ with $g(t)=f(x+tv)$
convex in $t\ \forall\ x,v \in \mathbb{R}^{n}$
$\rightarrow f$ convex (restriction to a line)

%TODO: Level set

%TODO: Example convex funcions

\subsubsection{Operations that preserve convexity (functions)}

- the point wise maximum of convex functions is convex

- the sum of convex functions is convex

- $f(Ax+b)$ is convex if $f$ is convex

\subsection{Convex Optimization Problems}

%TODO:Definition cvOP

Optimal value $f^\star =
	\operatorname{inf}\{f(x)\mid
	g_i(x)\le0,h_j=0 \}$
$f^\star=+\infty$ OP is infeasible,
$f^\star=-\infty$ OP is unbound below

%TODO: Local and Global Optimality

%TODO: Equivalent  OP, Slack

\subsubsection{Linear Programming}
$ \operatorname{minimize} c\T x
	\text{ s.t. } Ax-b \ge 0,\ x\ge0$

Step 1:
$\mathcal{L}(x,\lambda_1,\lambda_2) =
	c\T x-\lambda_1\T (Ax-b) -\lambda_2\T x,\ \lambda_i \ge0$

Step 2:
$\underset{x \in \mathcal{\mathbb{R}}^n}{\operatorname{inf}}
	\mathcal{L}=\lambda_1\T b$
, if $c-A\T\lambda_1-\lambda_2=0$, else $-\infty$

Step 3: Dual,
maximize $b\T\lambda$
s.t.
$c-A\T\lambda\ge0,\lambda\ge0$
(again LP)

\subsubsection{Quadratic Programming}

\subsection{Optimality Conditions}

\begin{sstTitleBox}{\textbf{\large
			Lagrange Duality
		}}
	\begin{centering}
		We consider ...
		\small
		% \vspace{-3mm}
		\begin{equation}
			f^\star = \inf_{x\in\mathcal{\mathbb{R}}^n}f(x)
			\;\text{ s.t. }\ g(x)\le0,h(x)=0
			\label{eq:dual}
		\end{equation}
		%
		% \vspace{-7mm}
		%TODO: Definitions lambda...

		\begin{sstFrame}
			{\color{white}
				\vspace{-3mm}
				\[\begin{aligned}
						 & \textbf{Lagrange}      & \hspace{-6mm}	\mathcal{L}(x,\lambda,\nu) & = f(x) + \lambda\T g(x)+\nu\T h(x)
						\\
						 & \textbf{Dual Function} & \hspace{-6mm}		d(\lambda,\nu)            & = \inf_{x \in \mathcal{\mathbb{R}}^n}\mathcal{L}(x,\lambda,\nu)
					\end{aligned}\]
				\vspace{-4mm}
			}
		\end{sstFrame}
	\end{centering}
\end{sstTitleBox}

$$\begin{aligned}
		                           & \textbf{Lagrange }                                              &
		\mathcal{L}(x,\lambda,\nu) & = f(x) + \lambda\T g(x)+\nu\T h(x)
		\\
		                           & \textbf{Dual Function }                                         &
		d(\lambda,\nu)             & = \inf_{x \in \mathcal{\mathbb{R}}^n}\mathcal{L}(x,\lambda,\nu)
	\end{aligned}$$

%TODO: Primal Dual (P) (D)

%TODO: Example LP

%TODO: Example QP

\subsubsection{Weak and Strong Duality}

\begin{proposition}[Weak Duality]
	$d(\lambda,\nu)\le f^\star,\forall\lambda\ge0,\nu\in\mathbb{R}^{h}$
\end{proposition}

\begin{definition}[Constraint qualification]
	$\mathcal{C}$ convex, \textbf{Slaters Condition} holds if
	$\exists\ \hat{x} \in \mathbb{R}^{n}$ s.t. $h(\hat{x})=0$ and $g(\hat{x})<0$
\end{definition}

\begin{proposition}[Strong Duality]
	If Slater's condition holds
	and (\ref{eq:dual}) is convex
	$\Rightarrow$
	$\exists \lambda \ge 0, \nu \in \mathbb{R}^{n_h}$ s.t. $d(\lambda,\nu)=f^\star$
\end{proposition}



\begin{sstTitleBox}[BrickRed]{\textbf{\large
			KKT  Conditions}
		\normalsize(Karush-Kuhn-Tucker)}
	\begin{theorem}[KKT Conditions]
		\begin{centering}
			If Slater's condition holds and
			(\ref{eq:dual}) is convex
			$\rightarrow$
			$x^\star \in \mathbb{R}^{n}$ is a minimizer of the primal (\ref{eq:dual})
			and $(\lambda^\star\ge0,\nu^\star)\in\mathbb{R}^{n_g}\times\mathbb{R}^{n_h}$
			is a maximizer of the dual $\Leftrightarrow$
			is equivalent to the following statements:
			\begin{sstFrame}[BrickRed]
				\vspace{-3mm}
				\color{white}
				\small
				\[\begin{aligned}
						\textbf{KKT-1 } & \text{\footnotesize (Stationary Lagrangian)} \hspace{-2mm} &  & \nabla_x\mathcal{L}(x^\star,\lambda^\star,\nu^\star)  =  0
						\\
						\textbf{KKT-2 } & \text{\footnotesize (primal feasibility)}                  &  & g(x^\star)\le0, h(x^\star)                            =                            0
						\\
						\textbf{KKT-3 } & \text{\footnotesize (dual feasibility)}                    &  & \lambda^\star  , \nu^\star \in \mathbb{R}^{n_h}       \ge       0
						\\
						\textbf{KKT-4 } & \text{\footnotesize (compementary}                         &  & \lambda^{\star T} g(x^\star)                          =                          0
						\\
						                & \text{\footnotesize slackness)}                            &  & \nu^{\star T} h(x^\star)                              =  0
					\end{aligned}\]
				\vspace{-4mm}
			\end{sstFrame}
			In addition we have:
			$\sup_{\lambda\ge0,\nu\in\mathbb{R}^{n_h}}q(\lambda,\nu)=\inf_{x\in\mathcal{C}}f(x)$
		\end{centering}
	\end{theorem}
\end{sstTitleBox}

\textbf{Remark} Without Slater,
KKT1-4 still implies $x^\star$ minimizes (\ref{eq:dual})
and $\lambda,\nu$ maximizes dual,
but the converse is no longer true.
There can be primal-minimizer/dual-maximizer not satisfy KKT.

%TODO Example KKT QP

%TODO: Sensitivity Analysis
