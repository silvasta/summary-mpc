\section{Introduction}

\textbf{Requirements for MPC}

1. A model of the system

2. A state estimator

3. Define the optimal control problem

4. Set up the optimization problem

5. Get the optimal control sequence

(solve the optimization problem)

6. Verify that the closed-loop system performs as desired
% %TODO: introtext


\subsection{Exact ODE solution of a Linear System}

\[
	x(t) = e^{A^c(t-t_0)}x_0 +
	\textstyle\int_{t_0}^{t}e^{A^c(t-\tau)}B^c u(\tau)d\tau
\]

\textbf{Problem} Most physical systems are nonlinear
% but linear systems are much better understood

\textbf{Idea} use First Order Taylor expansion
$f(\bar{x}) + \left. \frac{\partial f}{\partial x^\top} \right
	\rvert_{\bar{x}} (x-\bar{x})$

\subsection{Linearization}


\[\begin{aligned}
		\dot{x_s} & =g(x_s,u_s) = 0
		          & \Delta \dot{x}  =\dot{x} -\dot{x_s}
		          & = A^c\Delta x + B^c\Delta u
		\\
		y_s       & = h(x_s,u_s)
		          & \Delta y        = y - y_s
		          & = C\Delta x + D\Delta u
	\end{aligned} \]

%HACK: efficient ordering of values

$A^c= \left.\frac{\partial g}{\partial x^T}\right|_{\substack{x_s \\u_s}} $
$B^c= \left.\frac{\partial g}{\partial u^T}\right|_{\substack{x_s \\u_s}} $
$C= \left.\frac{\partial h}{\partial x^T}\right|_{\substack{x_s \\u_s}} $
$D= \left.\frac{\partial h}{\partial u^T}\right|_{\substack{x_s \\u_s}} $

%NOTE: Example linearization

\subsection{Discretization}

For general nonlinear systems only approximate discretization methods
exist, such as Euler, quality depends on sampling time

\begin{minipage}[c]{0.5\linewidth}
	\textbf{Approximation}\\
	\[
		\dot{x}^c \approx \frac{x^c(t + T_s)-x^c(t)}{T_s}
	\]
\end{minipage}
\begin{minipage}[c]{0.45\linewidth}
	\center\textbf{Notation}
	$x(k):= x^c(t_0+kT_s)$\\
	$u(k):= u^c(t_0+kT_s)$\\
\end{minipage}
\textbf{Exact Discretization of Linear Time-Invariant Models}
\[\begin{aligned}
		x(t_{k+1}) & =
		\underbrace{e^{A^c T_s}}
		_{=A} x(t_k) +
		\underbrace{\textstyle\int_{0}^{T_s}
			e^{A^c(T_s - \tau)}B^c d\tau}
		_{B=(A^c)^{-1}(A-\mathbb{I})B^c} u(t_k)
		\\
		x(k+N)     & = A^N x(k) + \textstyle\sum_{i=0}^{N-1}
		A^i B u(k+N-1-i)
	\end{aligned} \]

\subsection{Analysis of LTI Discrete-Time Systems}

%NOTE: Coordinate  Transform


\textbf{Controllabe}
$
	\text{if rank}(\mathcal{C}) = n
	,\;
	\mathcal{C} =
	\begin{bmatrix}
		B & \cdots & A^{n-1} B
	\end{bmatrix}
$

$\forall(x(0),x^*)\exists$ finite time $N$
with inputs $\mathcal{U}$, s.t. $x(N)=x^*$

\textbf{Stabilizable} iff all uncontrollable modes stable

\textbf{Observable}
$
	\text{if rank}(\mathcal{O}) = n
	,\;
	\begin{bmatrix}
		C^\top & \cdots & (CA^{n-1})^\top
	\end{bmatrix}^\top
$

$\forall x(0)\exists$ finite time $N$, s.t. the measurements

$y(0), \dots, y(N-1)$
uniquely distinguish initial state $x(0)$

\textbf{Detectablitiy} iff all unobservable modes stable

\subsection{Lyapunov}

Stability is a property of an
\textbf{equilibrium point} $\bar{\mathbf{x}}$
of a system

\begin{definition}[Lyapunov Stability]
	$\bar{\mathbf{x}}$ is \textbf{Lyapunov stable} if:

	$\forall\ \epsilon>0\ \exists\ \delta(\epsilon)$ s.t.
	$\lvert\lvert x(0) - \bar{x} \rvert\rvert < \delta(\epsilon) \to
		\lvert\lvert x(k) - \bar{x} \rvert\rvert < \epsilon$
\end{definition}

\begin{definition}[Globally asymptotic stability]
	If $\bar{\mathbf{x}}$ is Lyapunov stable and attractive, i.e.,
	$\lim_{k\to\infty} ||x(k)-\bar{x}||=0,\ \forall x(0)$
	then $\bar{\mathbf{x}}$ is \textbf{globally asymptotic stable}.
\end{definition}

\begin{sstFrame}[RoyalBlue!50]
	\begin{definition}[Global Lyapunov function]
		For  $\bar{\mathbf{x}}=0$,
		function $V:\mathbb{R}^n\to \mathbb{R}$ is called
		\textbf{Lyapunov function} if it is continuous at the origin,
		finite $\forall\ x\in \mathbb{R}^{n}$,
		\[
			||x||\to\infty\Rightarrow V(x)\to\infty
		\]
		\[
			V(x)>0\ \forall\ x\in\mathbb{R}^n \setminus\{0\}\quad V(0)=0
		\]
		\[
			V(g(x)) - V(x) \leq -\alpha(x) \quad \forall x \in \mathbb{R}^n
		\]

		where $\alpha:\mathbb{R}^n\to \mathbb{R}$
		continuous positive definite
	\end{definition}
\end{sstFrame}

\begin{sstTitleBox}[Plum]{\center\textbf{\large
			Lyapunov Theorem
		}}
	\begin{theorem}
		%TODO: non linear system (1)
		If a system admits a Lyapunov function $V(x)$,
		then $\bar{\mathbf{x}} = 0$ is
		\textbf{globally asymptotically stable}.
	\end{theorem}

	%WARN: Lyapunov, DIRECT?, indirect, check  L15.p11

	\begin{theorem}[Lyapunov indirect method]
		For linearization of system around $\bar{\mathbf{x}}=0$
		and resulting matrix
		$A=\left.\frac{\partial g}{\partial x^T}\right|_{x=0}$
		with eigenvalues

		$|\lambda_i| :=
			\begin{cases}
				\forall i := |\lambda_i| < 1 & \text{{x=0} is asymptotically stable} \\
				\exists i := |\lambda_i| > 1 & \text{origin is unstable}             \\
				\exists i := |\lambda_i| = 1 & \text{no info about stability}        \\
			\end{cases}$
	\end{theorem}
	%FIX: indirect Lyapunov

	\textbf{Discrete-Time Lyapunov equation}

	\[A^TPA-P=-Q,\quad Q>0\]

	\begin{theorem}[Existence of solution of DT Lyapunov equation]
		The discrete-time Lyapunov equation (3) has a unique solution P > 0 if and
		only if A has all eigenvalues inside the unit circle, i.e. if and only if the system
		x(k + 1) = Ax(k) is stable.
		%FIX: Lyapunov theorem3
	\end{theorem}
\end{sstTitleBox}

\subsection{Optimal Control}

%TODO: box for: Unconstrained Finite Horizon Control Problem

Unconstrained Finite Horizon Control Problem

$$\begin{aligned}
		J^\star(x(0)) :=
		\min_U & x_N^\top P x_N +
		\textstyle\sum_{i=0}^{N-1}
		x_i^\top Q x_i + u_i^\top R u_i
		\\
		\text{subject to  }
		       & x_{i+1}                = Ax_i+Bu_i
		\quad i  = 0,\dots,N-1                      \\
		       & x_0                    = x(0)
	\end{aligned}$$

$P\succeq0$, with $P=P^T$
terminal weight

$Q\succeq0$, with $Q = Q^T$
state weight

$R\succ0$, with $R = R^T$
input weight

\subsection{Batch Approach}

expresses cost function in terms of $x(0)$ and input sequence $U$

\[ \begin{bmatrix}
		x_0    \\
		x_1    \\
		\vdots \\
		x_N
	\end{bmatrix}
	=
	\begin{bmatrix}
		\mathbb{I} \\
		A          \\
		\vdots     \\
		A^N
	\end{bmatrix}
	x(0) +
	\begin{bmatrix}
		0        & \cdots & 0 \\
		B        & 0      & 0 \\
		AB       & B      & 0 \\
		\vdots   & \ddots & 0 \\
		A^{N-1}B & \cdots & B
	\end{bmatrix}
	\begin{bmatrix}
		u_0    \\
		u_1    \\
		\vdots \\
		u_{N-1}
	\end{bmatrix} \]


$\overline{Q} := \mathop{\mathrm{blockdiag}}(Q,\dots, Q,P)$
\quad
$\overline{R} := \mathop{\mathrm{blockdiag}}(R,\dots, R)$

\textbf{Optimal Input}
(from $\nabla_UJ(x(0),U)=2HU+2F^\top x(0)=0$)
%TODO: maybe derivation?
%WARN: definition S, F unclear
\[ U^\star(x(0)) =
	- \bigl(
	\underbrace{
			(\mathcal{S}^u)^\top \overline{Q} \mathcal{S}^u + \overline{R}
		}_{H\text{(Hessian)}^{-1}}
	\bigr)
	\underbrace{
		(\mathcal{S}^u)^\top \overline{Q}\mathcal{S}^x
	}_ {F^\top}
	x(0)
\]


\textbf{Optimal Cost}

\scalebox{0.97}[1]{$
		\scriptstyle
		J^\star(x(0)) = x(0)^\top (
		\mathcal{S}_x^\top \overline{Q} \mathcal{S}_x
		- \mathcal{S}_x^\top \overline{Q} \mathcal{S}_u
		(\mathcal{S}_u^\top \overline{Q} \mathcal{S}_u
		+ \overline{R})^{-1}
		\mathcal{S}_u^\top \overline{Q} \mathcal{S}_x
		)x(0)$}

\subsection{Recursive Approach}

uses dynamic programming to solve problem backwards from $N$

\[\begin{aligned}
		J_j^\star(x(j)) :=
		\min_{U_{j\to N}} & x_N^\top P x_N \!+
		\sum_{i=j}^{N-1}x_i^\top Q x_i + u_i^\top R u_i
	\end{aligned}\]

\begin{sstTitleBox}[ForestGreen]{\textbf{\large
			Ricatti Equations
		}
		% small text
	}


	\begin{sstTitleBox}[ForestGreen]
		% \vspace{-1.5mm}
		{\color{white}\textbf{RDE} - Riccati Difference Equation}

		\scalebox{0.97}[1]{$
				\scriptstyle
				P_i = A^\top P_{i+1} A
				+ Q - A^\top P_{i+1} B
				(B^\top P_{i+1} B + R)^{-1}
				B^\top P_{i+1} A$}
		% \vspace{-2.5mm}
	\end{sstTitleBox}

	\begin{sstFrame}[ForestGreen]
		% \vspace{-1.5mm}
		\color{white}
		{\textbf{RDE - Riccati Difference Equation} solved recursively}
		\scalebox{0.97}[1]{$
				\scriptstyle
				P_i = A^\top P_{i+1} A
				+ Q - A^\top P_{i+1} B
				(B^\top P_{i+1} B + R)^{-1}
				B^\top P_{i+1} A$}
		% \vspace{-2.5mm}
	\end{sstFrame}

	\begin{sstFrame}[ForestGreen]
		% \vspace{-1.5mm}
		\color{white}
		{\textbf{ARE - Algebraic Riccati Equation} solved analytically}
		{\center$\scriptstyle
				P_\infty = A^\top P_\infty A
				+ Q - A^\top P_\infty B
				(B^\top P_\infty B + R)^{-1}
				B^\top P_\infty A$}
		% \vspace{-2.5mm}
	\end{sstFrame}

\end{sstTitleBox}

\begin{minipage}[t]{0.64\linewidth}
	\textbf{From Principle Of Optimality}
	\[
		J_j^\star(x_j) =
		\min_{u_j} I(x_i,u_i) + J_{j+1}^\star(x_{j+1})
	\]
\end{minipage}
\begin{minipage}[t]{0.32\linewidth}
	\textbf{Optimal Cost-To-Go}
	\[
		J_i^\star(x_i) = x_i^\top P_i x_i
	\]
\end{minipage}

\textbf{Optimal Control Policy}
\[
	u_i^\star = F_i x_i =
	-(B^\top P_{i+1}B + R)^{-1}
	B^\top P_{i+1} A \cdot x(i)
\]

\subsection{Comparison of Batch and Recursive Approaches}

Batch optimization returns sequence $U^\star(x(0))$
of \textbf{numeric values} depending only on x(0),
dynamic programming yields \textbf{feedback policies}
$u_i^\star = F_i x_i$ depending on each $x_i$.

\textbf{Choice of P}

1. Match infinite solution, use ARE

2. Assume no control needed after N, use Lyapunov Equation %TODO: Lyapunov link
(makes only sense when asymptotically stable, otherwise P not positive definite)

3. set constraint $x_{i+N}=0$

\subsection{Infinite Horizon LQR}
\begin{sstTitleBox}[ForestGreen]{\center\textbf{\large
			LQR
		}
		% small text
	}

	\[ \begin{aligned}
			J_\infty^\star(x(k))  & = \min \sum_{i=0}^\infty
			x_i^\top Q x_i + u_i^\top R u_i                                   \\
			\mathrm{subj.\ to }\  & x_{i+1} = Ax_i + Bu_i,\quad    x_0 = x(k)
		\end{aligned} \]

	Same u as for finite problem but with ARE
	Constant  Feedback Matrix $F\infty$
	asymptotically stable for.. Q,R,stabi,detect
	%TODO: details  LQR

	%HACK: Lemma  Lyapunov (look at proof)

\end{sstTitleBox}

\subsection{Optimization}

A mathematical optimization problem is generally formulated as:

\begin{sstTitleBox}[Plum]{\textbf{\large
			Mathematical Optimization Problem
		}}
	\begin{minipage}{0.70\linewidth}
		\begin{sstOnlyFrame}[Plum]
			\textbf{Decision variable} $x  \in \mathbb{R}^{n}$

			\textbf{Objectivce function} $f: \dom(f)\to\mathbb{R}$

			\textbf{Inequality constraints} $g_i$
			$\scriptstyle(i \in \#\text{constraints})$

			\textbf{Equality constraints} $h_i$
			$\scriptstyle(i \in \#\text{constraints})$

			\textbf{Fesabile set}
			$\mathcal{X}\!\!:=\!\!\{x|g(x)\!\!\le\!\!0,\!h(x)\!=\!0\}$
		\end{sstOnlyFrame}
	\end{minipage}
	\begin{minipage}{0.28\linewidth}
		\begin{sstFullFrame}[Plum]
			{\color{white}
				\vspace{-1mm}
				\[ \begin{aligned}
						 & \textbf{minimize }f(x)   \\
						 & \ \ \ \text{subject to:} \\
						 & \ \ \ g_i(x)  \le 0      \\
						 & \ \ \ h_i(x)  = 0
					\end{aligned} \]
				\vspace{-1mm}
			}
		\end{sstFullFrame}
	\end{minipage}

	\begin{sstOnlyFrame}[Plum]
		\textbf{Feasible point}
		$x\in\dom(f)$ with
		$g_i(x)\le 0,\ h_i(x)=0$

		\textbf{Strictly feasible point}
		%WARN: what  if no Inequality constraints?
		$x$ with strict inequality
		$g_i(x)<0$

		\textbf{Optimal value}
		$f^\star (\text{or } p^\star)=
			\inf\{f(x)|
			g_i(x)\le0,h_j=0 \}$
		$f^\star=+\infty$: OP infeasible,
		$f^\star=-\infty$: OP unbound below

		\textbf{Optimizer}
		% (is set)
		set:
		$\argmin_{x \in \mathcal{X}} f(x):=
			\{ x\in\mathcal{X}|f(x)=f^\star\}$
	\end{sstOnlyFrame}

	\begin{sstOnlyFrame}[Plum]
		$x^\star$ is a \textbf{Global Minimum} if $f(x^\star)\leq f(x)$

		$x^\star$ is a \textbf{Local Minimum} if
		$\exists\ \epsilon > 0$ s.t.
		$f(x^\star)\leq f(x)$
		$\forall x \in \mathcal{X} \cap B_\epsilon(x^\star)$,
		open ball with center $x^\star$ and radius $\epsilon$
	\end{sstOnlyFrame}
\end{sstTitleBox}
\subsection{Convex Sets}

\begin{definition}[Convex Set]
	Set $\mathcal{C}$ is convex if and only if
	\[\theta x + (1-\theta)y \in \mathcal{C},
		\ \forall\ x,y \in \mathcal{C},
		\ \forall\ \theta \in [0,1]\]
\end{definition}

\begin{definition}[Hyperplanes]
	$\{x \in \mathbb{R}^n \mid a\T x=b\}$
\end{definition}
\begin{definition}[Halfspaces]
	$\{x \in \mathbb{R}^n \mid a\T x\le b\}$

	can be \textbf{open} (strict inequality)
	or \textbf{closed} (non-strict inequality)
\end{definition}

\begin{definition}[Polyhedra] intersection of
	\textbf{finite} number of closed halfspaces:
	polyhedra $\{x\in\mathbb{R}^n\mid A^{q\times n}x\preceq b^{q\times1},
		% C^{r\times n}x=d^{r\times1}
		\}$
\end{definition}

\begin{definition}[Polytope]
	is a \textbf{bounded} polyhedron.
\end{definition}

\begin{definition}[Convex hull]
	for  $\{v_1,...,v_k\}\in \mathbb{R}^{d}$ is:

	co$(\{v_1,...,v_k\}):=
		\{ x|x=\sum_{i}\lambda_iv_i,
		\lambda\ge0, \sum_{i}\lambda_i=1 \}$
\end{definition}

\begin{definition}[Ellipsoid]
	set:
	$\{ x | (x\!-\!x_c)^\top A^{-1}(x\!-\!x_c) \leq 1 \}$
	where $x_c$ is center of ellipsoid,
	$A \succ 0$ (i.e. positive definite)
	(Semi-axis lengths are square roots of eigenvalues of $A$)
\end{definition}

\begin{definition}[Norm Ball]
	$B_r(x):=
		\{\xi\in\mathbb{R}^{n}:|\xi-x|_p<r\}$
	where $p$ defines the $l_p$ norm, $p=\{1|2|..|\infty\}$
	%TODO: norm p1,2,...
\end{definition}

%NOTE:intersection theorem? proof?
\textbf{Intersection}
$\mathcal{C}_1, \mathcal{C}_2$ cv
$\Rightarrow \mathcal{C}_1 \cap \mathcal{C}_2$ convex \textbf{(cv)}

\textbf{Image under affine map}
$\mathcal{C} \subseteq  \mathbb{R}^{n}$ cv
$\Rightarrow \{Ax+b \mid x \in \mathcal{C} \}$ cv

\textbf{Inverse IoaM}
$\mathcal{C} \subseteq  \mathbb{R}^{m}$ cv
$\Rightarrow \{x\in\mathbb{R}^{n} \mid  Ax+b\in\mathcal{C}\}$ cv

\subsection{Convex Functions}

\begin{definition}[Convex Function]
	$f: \mathcal{C}_{cv} \to\mathbb{R}$ is convex iff
	\[
		f(\theta x + (1-\theta)y)\le \theta f(x)+ (1-\theta)f(y),
		\ \forall\ x,y \in \mathcal{C},
		\ \forall\ \theta \in [0,1]\]
	$f$ is strictly convex if this inequality is strict.
\end{definition}


\begin{definition}[Epigraph]
	$f:\mathbb{R}^n \rightarrow \mathbb{R}$ cv
	$\Leftrightarrow$
	epi$(f)$ is cv set
	$$\operatorname{epi}(f):=\{(x,t)\in \mathbb{R}^{n+1} | f(x)\le t\}$$
\end{definition}

\textbf{Check Convexity} $f$ is convex if it is
composition of simple convex function
with convexity preserving operations
or if


$f: \mathbb{R}^n \rightarrow \mathbb{R}$ twice differentiable,
$\partial^2f/\partial x^2 \succeq 0\ \forall\ x \in \mathbb{R}^{n}$

$g: \mathbb{R} \rightarrow \mathbb{R}$ with $g(t)=f(x+tv)$
convex in $t\ \forall\ x,v \in \mathbb{R}^{n}$
$\rightarrow f$ convex (restriction to a line)

%NOTE: first, second order convexity


- the point wise maximum of convex functions is convex

- the sum of convex functions is convex

- $f(Ax+b)$ is convex if $f$ is convex

%TODO: Level set sublevel set

%NOTE: Example convex funcions

%NOTE:Definition cvOP

\begin{theorem}
	For a convex optimization problem,
	\textbf{any} locally optimal solution is globally
	optimal (local optima are global optima).
\end{theorem}


%TODO: Equivalent OP, Slack

\textbf{Linear Programming}
$ \operatorname{minimize} c\T x
	\text{ s.t. } Ax-b \ge 0,\ x\ge0$

Step 1:
$\mathcal{L}(x,\lambda_1,\lambda_2) =
	c\T x-\lambda_1\T (Ax-b) -\lambda_2\T x,\ \lambda_i \ge0$
%TODO: LP
Step 2:
$\underset{x \in \mathcal{\mathbb{R}}^n}{\operatorname{inf}}
	\mathcal{L}=\lambda_1\T b$
, if $c-A\T\lambda_1-\lambda_2=0$, else $-\infty$

Step 3: Dual,
maximize $b\T\lambda$
s.t.
$c-A\T\lambda\ge0,\lambda\ge0$
(again LP)

\textbf{Quadratic Programming}
min ...
%TODO: QP

\subsection{Optimality Conditions}

\begin{sstTitleBox}{\center\textbf{\large
			Lagrange Duality
		}}
	\begin{centering}
		\begin{sstOnlyFrame}
			\vspace{-1.5mm}
			\[ \text{Consider}\ \
				f^\star = \inf_{x\in\mathcal{\mathbb{R}}^n}f(x)
				\;\text{ s.t. }\ g(x)\le0,\ h(x)=0
				\label{eq:dual}%FIX: label for Optimality Conditions
			\]
			% \vspace{-7mm}
		\end{sstOnlyFrame}

		%TODO: Definitions lambda...

		\begin{sstFrame}
			{\color{white}
				\vspace{-2mm}
				\[\begin{aligned}
						 & \textbf{Lagrangian}
						 & \hspace{-4mm}	\mathcal{L}(x,\lambda,\nu)
						 & = f(x) + \lambda\T g(x)+\nu\T h(x)
						\\
						\\
						 & \textbf{Dual Function}
						 & \hspace{-4mm}		d(\lambda,\nu)            & = \inf_{x \in \mathcal{\mathbb{R}}^n}\mathcal{L}(x,\lambda,\nu)
					\end{aligned}\]
				\vspace{-2.7mm}
			}
		\end{sstFrame}
	\end{centering}
\end{sstTitleBox}


%HACK: Primal Dual (P) (D)

%TODO: Primal dual Example LP, Example QP


\begin{proposition}[Weak Duality]
	$d(\lambda,\nu)\le f^\star,\forall\lambda\ge0,\nu\in\mathbb{R}^{h}$
\end{proposition}

\begin{definition}[Constraint qualification]
	\textbf{Slaters Condition} holds if $\exists$
	at least one
	strictly feasible point
	% \textbf{strictly feasible point}
	$\hat{x}{\ (h(\hat{x})=0,\ g(\hat{x})<0)}$
\end{definition}

% \begin{definition}[Constraint qualification]
% 	\textbf{Slaters Condition} holds if $\exists$
% 	strictly feasible point
% 	% \textbf{strictly feasible point}
% 	$\hat{x}:= (h(\hat{x})=0$ and $g(\hat{x})<0$)
% \end{definition}

\begin{proposition}[Strong Duality]
	If Slater's condition holds
	and OP is convex
	$\Rightarrow$
	$\exists \lambda \ge 0, \nu \in \mathbb{R}^{n_h}$ s.t. $d(\lambda,\nu)=f^\star$
\end{proposition}



\begin{sstTitleBox}[BrickRed]{\textbf{\large
			KKT  Conditions}
		\normalsize(Karush-Kuhn-Tucker)}
	\begin{theorem}[KKT Conditions]
		\begin{centering}
			If Slater's condition holds and
			(\ref{eq:dual}) is convex
			$\rightarrow$
			$x^\star \in \mathbb{R}^{n}$ is a minimizer of the primal (\ref{eq:dual})
			and $(\lambda^\star\ge0,\nu^\star)\in\mathbb{R}^{n_g}\times\mathbb{R}^{n_h}$
			is a maximizer of the dual $\Leftrightarrow$
			is equivalent to the following statements:
			\begin{sstFrame}[BrickRed]
				\vspace{-3mm}
				\color{white}
				\small
				\[\begin{aligned}
						\textbf{KKT-1 } & \text{\footnotesize (Stationary Lagrangian)} \hspace{-2mm} &  & \nabla_x\mathcal{L}(x^\star,\lambda^\star,\nu^\star)  =  0
						\\
						\textbf{KKT-2 } & \text{\footnotesize (primal feasibility)}                  &  & g(x^\star)\le0, h(x^\star)                            =                            0
						\\
						\textbf{KKT-3 } & \text{\footnotesize (dual feasibility)}                    &  & \lambda^\star  , \nu^\star \in \mathbb{R}^{n_h}       \ge       0
						\\
						\textbf{KKT-4 } & \text{\footnotesize (compementary}                         &  & \lambda^{\star T} g(x^\star)                          =                          0
						\\
						                & \text{\footnotesize slackness)}                            &  & \nu^{\star T} h(x^\star)                              =  0
					\end{aligned}\]
				\vspace{-4mm}
			\end{sstFrame}
			In addition we have:
			$\sup_{\lambda\ge0,\nu\in\mathbb{R}^{n_h}}q(\lambda,\nu)=\inf_{x\in\mathcal{C}}f(x)$
		\end{centering}
	\end{theorem}
\end{sstTitleBox}

\textbf{Remark} Without Slater,
KKT1-4 still implies $x^\star$ minimizes (\ref{eq:dual})
and $\lambda,\nu$ maximizes dual,
but the converse is no longer true.
There can be primal-minimizer/dual-maximizer not satisfy KKT.

%TODO Example KKT QP

%TODO: Sensitivity Analysis
