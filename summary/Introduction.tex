\section{Introduction}

\subsection{Exact  Solution}

$x(t) = e^{A^c(t-t_0)}x_0 +
	\textstyle\int_{t_0}^{t}e^{A^c(t-\tau)}B^c u(\tau)d\tau$


\subsection{Linearization}

%TODO: efficient ordering of values
$\dot{x_s}=g(x_s,u_s) = 0$
$y_s= h(x_s,u_s)$

$\Delta \dot{x} =\dot{x} -\dot{x_s} = A^c\Delta x + B^c\Delta u$

$\Delta y = y - y_s  = C\Delta x + D\Delta u $

$A^c= \left.\frac{\partial g}{\partial x^T}\right|_{\substack{x_s \\u_s}} $
$B^c= \left.\frac{\partial g}{\partial u^T}\right|_{\substack{x_s \\u_s}} $
$C= \left.\frac{\partial h}{\partial x^T}\right|_{\substack{x_s \\u_s}} $
$D= \left.\frac{\partial h}{\partial u^T}\right|_{\substack{x_s \\u_s}} $
%TODO: Example linearization

\subsection{Discretization}
%
%TODO: Euler

\subsection{Lyapunov}
%TODO: Lyapunov
%TODO: Lyapunov direct, indirect

\subsection{Optimal Control}

Unconstrained Finite Horizon Control Problem

$$\begin{aligned}
		J^\star(x(0)) :=
		\min_U & x_N^\top P x_N +
		\textstyle\sum_{i=0}^{N-1}
		x_i^\top Q x_i + u_i^\top R u_i
		\\
		\text{subject to  }
		       & x_{i+1}                = Ax_i+Bu_i
		\quad i  = 0,\dots,N-1                      \\
		       & x_0                    = x(0)
	\end{aligned}$$

\subsection{Batch Approach}

$$ \begin{bmatrix}
		x_0    \\
		x_1    \\
		\vdots \\
		x_N
	\end{bmatrix}
	=
	\begin{bmatrix}
		\mathbb{I} \\
		A          \\
		\vdots     \\
		A^N
	\end{bmatrix}
	x(0) +
	\begin{bmatrix}
		0        & \cdots & 0 \\
		B        & 0      & 0 \\
		AB       & B      & 0 \\
		\vdots   & \ddots & 0 \\
		A^{N-1}B & \cdots & B
	\end{bmatrix}
	\begin{bmatrix}
		u_0    \\
		u_1    \\
		\vdots \\
		u_{N-1}
	\end{bmatrix} $$

\textbf{Cost}

$\overline{Q} := \mathop{\mathrm{blockdiag}}(Q,\dots, Q,P)$

$\overline{R} := \mathop{\mathrm{blockdiag}}(R,\dots, R)$

\textbf{Optimal Input}
$$ U^\star(x(0)) =
	- \bigl(
	\underbrace{
			(\mathcal{S}^u)^\top \overline{Q} \mathcal{S}^u + \overline{R}
		}_{H\text{(Hessian)}^{-1}}
	\bigr)
	\underbrace{
		(\mathcal{S}^u)^\top \overline{Q}\mathcal{S}^x
	}_ {F^\top}
	x(0)
$$
%TODO Derivation batch
\textbf{Optimal Cost}

\scalebox{0.97}[1]{$
		\scriptstyle
		J^\star(x(0)) = x(0)^\top (
		\mathcal{S}_x^\top \overline{Q} \mathcal{S}_x
		- \mathcal{S}_x^\top \overline{Q} \mathcal{S}_u
		(\mathcal{S}_u^\top \overline{Q} \mathcal{S}_u
		+ \overline{R})^{-1}
		\mathcal{S}_u^\top \overline{Q} \mathcal{S}_x
		)x(0)$}

\subsection{Recursive Approach}

%TODO: Recursive Approach


\subsection{Infinite Horizon LQR}
%TODO: LQR

\section{Optimization}

%TODO: epigraph

A mathematical optimization problem is generally formulated as:

\begin{equation}
	\begin{aligned}
		\min_{x\in\operatorname{dom}(f)}          & f(x)                      \\
		\text{s.t. }                       g_i(x) & \le 0,\quad i = 1,\dots,m \\
		h_i(x)                                    & = 0,\quad i = 1,\dots,p   \\
	\end{aligned}
	\label{eq:optimization}
\end{equation}

- $x = (x_1,...,x_n) \in \mathbb{R}^{n}$ decision variable

- $f: \operatorname{dom}(f)\to\mathbb{R}$ objectivce function

- $\mathcal{X} = \{x \in \mathbb{R}^{n}: g(\xi)\le0,\ h(\xi)=0\}$ fesabile set

%TODO: further definitions

\subsection{Convex Sets}

\begin{definition}[Convex Set]
	A set $\mathcal{C}$ is convex if and only if
	$$\theta x + (1-\theta)y \in \mathcal{C}
		\forall\ x,y \in \mathcal{C},\quad
		\forall\ \theta \in [0,1]$$
\end{definition}

(hyperplane $\parallel$ half-space)
$\{x \in \mathbb{R}^n \mid a\T x(=\parallel\le)b\}$

polyhedra $\{x\in\mathbb{R}^n\mid A^{q\times n}x\preceq b^{q\times1},C^{r\times n}x=d^{r\times1}\}$
%TODO: check

polytope

%TODO: Elipsoid

%TODO: Norm Ball

\textbf{Intersection}
$\mathcal{C}_1, \mathcal{C}_2$ cv
$\Rightarrow \mathcal{C}_1 \cap \mathcal{C}_2$ convex \textbf{(cv)}

\textbf{Image under affine map}
$\mathcal{C} \subseteq  \mathbb{R}^{n}$ cv
$\Rightarrow \{Ax+b \mid x \in \mathcal{C} \}$ cv

\textbf{Inverse IoaM}
$\mathcal{C} \subseteq  \mathbb{R}^{m}$ cv
$\Rightarrow \{x\in\mathbb{R}^{n} \mid  Ax+b\in\mathcal{C}\}$ cv

\subsection{Convex Functions}

%TODO: Convex Function Definition

\textbf{Check Convexity} $f$ is convex if it is
composition of simple convex function
with convexity preserving operations
or if


$f: \mathbb{R}^n \rightarrow \mathbb{R}$ twice differentiable,
$\partial^2f/\partial x^2 \succeq 0\ \forall\ x \in \mathbb{R}^{n}$

$g: \mathbb{R} \rightarrow \mathbb{R}$ with $g(t)=f(x+tv)$
convex in $t\ \forall\ x,v \in \mathbb{R}^{n}$
$\rightarrow f$ convex (restriction to a line)

%TODO: Level set

%TODO: Example convex funcions

\subsubsection{Operations that preserve convexity (functions)}

- the point wise maximum of convex functions is convex

- the sum of convex functions is convex

- $f(Ax+b)$ is convex if $f$ is convex

\subsection{Convex Optimization Problems}

%TODO:Definition cvOP

Optimal value $f^\star =
	\operatorname{inf}\{f(x)\mid
	g_i(x)\le0,h_j=0 \}$
$f^\star=+\infty$ OP is infeasible,
$f^\star=-\infty$ OP is unbound below

%TODO: Local and Global Optimality

%TODO: Equivalent  OP, Slack

\subsubsection{Linear Programming}
$ \operatorname{minimize} c\T x
	\text{ s.t. } Ax-b \ge 0,\ x\ge0$

Step 1:
$\mathcal{L}(x,\lambda_1,\lambda_2) =
	c\T x-\lambda_1\T (Ax-b) -\lambda_2\T x,\ \lambda_i \ge0$

Step 2:
$\underset{x \in \mathcal{\mathbb{R}}^n}{\operatorname{inf}}
	\mathcal{L}=\lambda_1\T b$
, if $c-A\T\lambda_1-\lambda_2=0$, else $-\infty$

Step 3: Dual,
maximize $b\T\lambda$
s.t.
$c-A\T\lambda\ge0,\lambda\ge0$
(again LP)

\subsubsection{Quadratic Programming}

\subsection{Optimality Conditions}

\begin{sstTitleBox}{\textbf{\large
			Lagrange Duality
		}}
	\begin{centering}
		We consider ...
		\small
		% \vspace{-3mm}
		\begin{equation}
			f^\star = \inf_{x\in\mathcal{\mathbb{R}}^n}f(x)
			\;\text{ s.t. }\ g(x)\le0,h(x)=0
			\label{eq:dual}
		\end{equation}
		%
		% \vspace{-7mm}
		%TODO: Definitions lambda...

		\begin{sstFrame}
			{\color{white}
				\vspace{-3mm}
				\[\begin{aligned}
						 & \textbf{Lagrange}      & \hspace{-6mm}	\mathcal{L}(x,\lambda,\nu) & = f(x) + \lambda\T g(x)+\nu\T h(x)
						\\
						 & \textbf{Dual Function} & \hspace{-6mm}		d(\lambda,\nu)            & = \inf_{x \in \mathcal{\mathbb{R}}^n}\mathcal{L}(x,\lambda,\nu)
					\end{aligned}\]
				\vspace{-4mm}
			}
		\end{sstFrame}
	\end{centering}
\end{sstTitleBox}

$$\begin{aligned}
		                           & \textbf{Lagrange }                                              &
		\mathcal{L}(x,\lambda,\nu) & = f(x) + \lambda\T g(x)+\nu\T h(x)
		\\
		                           & \textbf{Dual Function }                                         &
		d(\lambda,\nu)             & = \inf_{x \in \mathcal{\mathbb{R}}^n}\mathcal{L}(x,\lambda,\nu)
	\end{aligned}$$

%TODO: Primal Dual (P) (D)

%TODO: Example LP

%TODO: Example QP

\subsubsection{Weak and Strong Duality}

\begin{proposition}[Weak Duality]
	$d(\lambda,\nu)\le f^\star,\forall\lambda\ge0,\nu\in\mathbb{R}^{h}$
\end{proposition}

\begin{definition}[Constraint qualification]
	$\mathcal{C}$ convex, \textbf{Slaters Condition} holds if
	$\exists\ \hat{x} \in \mathbb{R}^{n}$ s.t. $h(\hat{x})=0$ and $g(\hat{x})<0$
\end{definition}

\begin{proposition}[Strong Duality]
	If Slater's condition holds
	and (\ref{eq:dual}) is convex
	$\Rightarrow$
	$\exists \lambda \ge 0, \nu \in \mathbb{R}^{n_h}$ s.t. $d(\lambda,\nu)=f^\star$
\end{proposition}



\begin{sstTitleBox}[BrickRed]{\textbf{\large
			KKT  Conditions}
		\normalsize(Karush-Kuhn-Tucker)}
	\begin{theorem}[KKT Conditions]
		\begin{centering}
			If Slater's condition holds and
			(\ref{eq:dual}) is convex
			$\rightarrow$
			$x^\star \in \mathbb{R}^{n}$ is a minimizer of the primal (\ref{eq:dual})
			and $(\lambda^\star\ge0,\nu^\star)\in\mathbb{R}^{n_g}\times\mathbb{R}^{n_h}$
			is a maximizer of the dual $\Leftrightarrow$
			is equivalent to the following statements:
			\begin{sstFrame}[BrickRed]
				\vspace{-3mm}
				\color{white}
				\small
				\[\begin{aligned}
						\textbf{KKT-1 } & \text{\footnotesize (Stationary Lagrangian)} \hspace{-2mm} &  & \nabla_x\mathcal{L}(x^\star,\lambda^\star,\nu^\star)  =  0
						\\
						\textbf{KKT-2 } & \text{\footnotesize (primal feasibility)}                  &  & g(x^\star)\le0, h(x^\star)                            =                            0
						\\
						\textbf{KKT-3 } & \text{\footnotesize (dual feasibility)}                    &  & \lambda^\star  , \nu^\star \in \mathbb{R}^{n_h}       \ge       0
						\\
						\textbf{KKT-4 } & \text{\footnotesize (compementary}                         &  & \lambda^{\star T} g(x^\star)                          =                          0
						\\
						                & \text{\footnotesize slackness)}                            &  & \nu^{\star T} h(x^\star)                              =  0
					\end{aligned}\]
				\vspace{-4mm}
			\end{sstFrame}
			In addition we have:
			$\sup_{\lambda\ge0,\nu\in\mathbb{R}^{n_h}}q(\lambda,\nu)=\inf_{x\in\mathcal{C}}f(x)$
		\end{centering}
	\end{theorem}
\end{sstTitleBox}

\textbf{Remark} Without Slater,
KKT1-4 still implies $x^\star$ minimizes (\ref{eq:dual})
and $\lambda,\nu$ maximizes dual,
but the converse is no longer true.
There can be primal-minimizer/dual-maximizer not satisfy KKT.

%TODO Example KKT QP

%TODO: Sensitivity Analysis
