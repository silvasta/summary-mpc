\chapter{Introduction}

\textbf{Requirements for MPC}

1. A model of the system

2. A state estimator

3. Define the optimal control problem

4. Set up the optimization problem

5. Get the optimal control sequence (solve the optimization problem)

6. Verify that the closed-loop system performs as desired

\section{System Theory Basics}

\subsection{Learning Objectives}

- Describe dynamics with continuous-time state-space models

- Derive linearized model and understand limitations of linear description

- Discretize nonlinear and linear systems and contrast model properties

- Analyse stability, controllability, observability of linear systems

- Understand Lyapunov stability and prove stability of nonlinear systems

- Construct a Lyapunov function for stable linear systems

\subsection{Model of Dynamic Systemes}

\textbf{Goal} Introduce mathematical models to be used in Model Predictive Control
(MPC) for describing the behavior of dynamic systems

\begin{table}[h]
	\begin{center}
		\caption{Model classification}\label{tab:}
		\begin{tabular}[c]{|l|l|}
			\hline
			state  space    & transfer funtion \\ \hline
			linear          & nonlinear        \\ \hline
			time-varying    & time-invariant   \\ \hline
			continuous-time & discrete-time    \\ \hline
			deterministic   & stochastic       \\ \hline
		\end{tabular}
	\end{center}
\end{table}

- If not stated differently, we use deterministic models

- Models of physical systems derived from first principles are mainly:

nonlinear, time-invariant, continuous-time, state space models (*)

- Target models for standard MPC are mainly:

linear, time-invariant, discrete-time, state space models (†)

- Focus of this section is on how to ’transform’ (*) to (†)

\subsubsection{Physical Model}

Nonlinear Time-Invariant Continuous-Time State Space Models

$\dot{x} = g(x,u)$

$y = h(x,u)$

$g(x,u): \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n$
system dynamics

$h(x,u): \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^p$
output funtion

$x \in \mathbb{R}^n$
state vector

$u \in \mathbb{R}^m$
input vector

$y \in \mathbb{R}^p$
output vector

- Very general class of models

- Higher order ODEs can be easily brought to this form

- Analysis and control synthesis generally hard
$\rightarrow$
linearization to bring it to linear, time-invariant (LTI),
continuous-time, state space form

\subsubsection{Continuous LTI-Model}

$\dot{x} = A^cx+B^cu$

$y = Cx+Du$

$A^c \in \mathbb{R}^{n \times n}$
system matrix

$B^c \in \mathbb{R}^{n \times m}$
input matrix

$C \in \mathbb{R}^{p \times n}$
output matrix

$D \in \mathbb{R}^{p \times m}$
troughput matrix

$x \in \mathbb{R}^n$
state vector

$u \in \mathbb{R}^m$
input vector

$y \in \mathbb{R}^p$
output vector

\textbf{Solution to linear ODEs}

$$
	x(t) = e^{A^c(t-t_0)}x_0 + \int_{t_0}^{t}e^{A^c(t-\tau)}B^c u(\tau)d\tau
$$

\textbf{Problem}
Most physical systems are nonlinear
but linear systems are much better understood

Nonlinear systems can be well approximated by a linear system
in a \textit{small neighborhood} around a point in state space

\textbf{Idea}
Control keeps the system around some operating point
$\rightarrow$
replace nonlinear by a linearized system around operating point

First Order Taylor expansion
$f(\bar{x}) + \left. \frac{\partial f}{\partial x^\top} \right
	\rvert_{\bar{x}} (x-\bar{x})$

\textbf{Linearization}

Stationary operating point:
$x_s,u_s$

$\dot{x_s} = g(x_s,u_s)=0$

$y_s = h(x_s,u_s)$

$$\begin{aligned}
		%%% x dot
		\dot{x} & =
		\underbrace{
			g(x_s,u_s)
		}_{=0}
		% A and x
		+
		\underbrace{
		\left.\frac{\partial g}{\partial x^T}\right|_{\substack{x_s \\u_s}}
		}_{=A^c}
		\underbrace{
			(x-x_s)
		}_{=\Delta x}
		% B and u
		+
		\underbrace{
		\left.\frac{\partial g}{\partial u^T}\right|_{\substack{x_s \\u_s}}
		}_{=B^c}
		\underbrace{
			(u-u_s)
		}_{=\Delta u}
		\\
		% conclusion
		\Rightarrow
		\dot{x} -
		\underbrace{
			\dot{x_s}
		}_{=0}
		        & = \Delta \dot{x}
		= A^c\Delta x + B^c\Delta u
		\\
		%%% y
		y       & =
		\underbrace{
			h(x_s,u_s)
		}_{y_s}
		% C
		+
		\underbrace{
		\left.\frac{\partial h}{\partial x^T}\right|_{\substack{x_s \\u_s}}
		}_{=C}
		(x-x_s)
		% D
		+
		\underbrace{
		\left.\frac{\partial h}{\partial u^T}\right|_{\substack{x_s \\u_s}}
		}_{=D}
		(u-u_s)
		\\
		% conclusion
		\Rightarrow
		y - y_s & = \Delta y
		= C\Delta x + D\Delta u
		\\
	\end{aligned}$$

Subsequently, instead of \Delta x, \Delta u and \Delta y , x, u and y are used for brevity.

\subsubsection{Discrete Models}

Discrete-Time systems are describey by difference equations

$x(k+1) = g(x(k),u(k))$

$y(k) = h(x(k),u(k))$

$g(x,u): \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n$
system dynamics

$h(x,u): \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^p$
output funtion

$x \in \mathbb{R}^n$
state vector

$u \in \mathbb{R}^m$
input vector

$y \in \mathbb{R}^p$
output vector

- Inputs and outputs of a discrete-time system are defined only at discrete time points,
i.e. its inputs and outputs are sequences defined for $k\in\mathbb{Z}$

- Discrete time systems describe either

1. Inherently discrete systems,
eg. bank savings account balance at the k-th month
$x(k + 1) = (1 + \alpha)x(k) + u(k)$

2. \textit{Transformed} continuous-time system

- Vast majority of controlled systems not inherently discrete-time systems

- Controllers almost always implemented using microprocessors

- Finite computation time must be considered in the control system design
$\rightarrow$ discretize the continuous-time system

- Discretization is the procedure of obtaining an ’equivalent’ discrete-time
system from a continuous-time system

- The discrete-time model describes the state of the continuous-time
system only at particular instances $t_k, k\in\mathbb{Z^+}$ in time, where
$t_{k+1} = t_k + T_s$ and $T_s$ is called the sampling time

- Usually $u(t) = u(t_k) \forall t \in [t_k,t_k+1)$ is assumed (and implemented)

\subsubsection{Discretization}

Approximate
$\dot{x}^c \approx \frac{x^c(t + T_s)-x^c(t)}{T_s}$

\subsubsection{Recap}

Continuous time systems:

- Continuous time state space models provide
framework for representing general class of models

- Analysis and control synthesis is generally hard

- Linearization of nonlinear system to
facilitate controller design and analysis

- Linearization only valid in a small neighborhood
of the linearization point

- Continuous time system requires system
integration to predict model behavior

Discrete time systems:

- Discrete time models most suited for MPC

- Solution of discrete time model is a linear
function of the initial state and the input sequence

- Zero-order-hold provides exact discretization
for linear systems

- For general nonlinear systems only approximate
discretization methods exist, such as Euler
discretization, quality depends on sampling time

\subsection{Analysis of Discrete-Time LTI-Systems}

\subsubsection{Coordinate Transform}
%NOTE: Coordinate  Transform

\subsubsection{Stability}
\textbf{Controllabe}
$
	\text{if rank}(\mathcal{C}) = n
	,\;
	\mathcal{C} =
	\begin{bmatrix}
		B & \cdots & A^{n-1} B
	\end{bmatrix}
$

$\forall(x(0),x^*)\exists$ finite time $N$
with inputs $\mathcal{U}$, s.t. $x(N)=x^*$

\textbf{Stabilizable} iff all uncontrollable modes stable

\subsubsection{Controllability}
\textbf{Observable}
$
	\text{if rank}(\mathcal{O}) = n
	,\;
	\begin{bmatrix}
		C^\top & \cdots & (CA^{n-1})^\top
	\end{bmatrix}^\top
$

$\forall x(0)\exists$ finite time $N$, s.t. the measurements

$y(0), \dots, y(N-1)$
uniquely distinguish initial state $x(0)$

\textbf{Detectablitiy} iff all unobservable modes stable

%NOTE: Detailed Controllability Observability

\subsection{Analysis of Discrete-Time Nonlinear-Systems}

\subsubsection{Lyapunov}

- We consider first the stability of a nonlinear,
time-invariant, discrete-time system
$x(k +1) = g(x(k))$
with an equilibrium point at x̄, i.e. g(x̄) = x̄.

- For nonlinear systems there are many definitions of stability.
- Informally, we define a system to be stable in the sense of Lyapunov, if it
stays in any arbitrarily small neighborhood of the equilibrium when it is
disturbed slightly.
- In the following we always mean “stability” in the sense of Lyapunov.
- Note that system (1) encompasses any open- or closed-loop autonomous
system.
- We will then derive simpler stability conditions for the specific case of LTI
systems.
- Note that stability is a property of an equilibrium point of a system.

%TODO: Lyapunov direct, indirect (copy)

Remarks

-  Note that the Lyapunov theorems only provide sufficient conditions

-  Lyapunov theory is a powerful concept for proving stability of a control
system, but for general nonlinear systems it is usually difficult to find a
Lyapunov function

-  Lyapunov functions can sometimes be derived from physical considerations

-  One common approach:

--  Decide on form of Lyapunov function (e.g., quadratic)

--  Search for parameter values e.g. via optimization so that the required
properties hold

-  Will see: MPC allows for a stability guarantee for constrained systems:
Lyapunov function is provided in the form of the optimal cost function
(under certain assumptions on the problem setup).

-  For linear systems there exist constructive theoretical results on the
existence of a quadratic Lyapunov function
%TODO: Lyapunov theorem 3

• Therefore, for LTI systems global asymptotic Lyapunov stability is not
only sufficient but also necessary, and it agrees with the notion of stability
based on eigenvalue location.

• Note that stability is always “global” for linear systems.

%NOTE: L1.p56 Property  of P

\subsection{Recap DT System}

-  Lyapunov stability provides framework to analyze stability of general
nonlinear systems

-  Asymptotic stability captures convergence in the limit and is commonly
used in MPC

-  For LTI systems, asymptotic stability can be shown by eigenvalues of the
dynamic matrix

-  Lyapunov functions provide means to prove stability for nonlinear systems

-  Key is how to find a Lyapunov function

-  For linear systems there exists a quadratic Lyapunov function if and only if
the system is stable

-  MPC allows to prove stability by providing a Lyapunov function for the
closed-loop system

-  Controllability/Observability are necessary requirements for design of a
controller/estimator (can be relaxed into stabilizability and detectability)

\section{Unconstrained LQ Optimal Control}

\subsection{Learning Objectives}

- Learn to compute finite horizon
unconstrained linear quadratic optimal controller in two ways

- Understand principle of optimality

- Learn to compute infinite horizon
unconstrained linear quadratic optimal controller

- Understand impact of horizon length

- Prove stability of infinite horizon
unconstrained linear quadratic optimal control

- Learn how to ‘simulate’ quasi-infinite horizon

\subsection{Introduction to Optimal Control}

\subsubsection{Optimal Control}
Discrete-time \textbf{optimal control} is concerned with
choosing an optimal input sequence
$U := [u_0^T, u_1^T,...]^T$ %FIX: u^T why?
(as measured by some objective function),
over a finite or infinite time horizon,
in order to apply it to a system with a given initial state x(0).

The objective, or cost, function is often defined as a
\textbf{sum of stage costs} $l(x_i,u_i)$and,
when the horizon has finite length N,
terminal cost $I_f(x_N)$:
$$
	J(x_0,U) :=  I_f(x_N) +
	\sum_{i=0}^{N-1}I((x_i,u_i)
$$
The states $\{x_i\}_{i=0}^N$ must satisfy the system dynamics

$$\begin{aligned}
		x_{i+1} & = g(x_i,u_i )
		\quad i = 0,\dots,N-1   \\
		x_0     & = x(0)
	\end{aligned}$$
and there may be state and/or input constraints
$$
	h(x_i,u_i)\le 0
	\quad i = 0,\dots,N-1
$$

In the finite horizon case,
there may also be a constraint that
the final state $x_N$ lies in a set $\mathcal{X}_f$

A general finite horizon optimal control formulation
for discrete-time systems is therefore

$$\begin{aligned}
		J^\star(x(0)) & :=  \min_{U}J(x(0),U)                    \\
		\text{subject to  }
		              & x_{i+1}                = g(x_i,u_i )
		\quad i  = 0,\dots,N-1                                   \\
		              & h(x_i,u_i)             \le 0
		\quad i  = 0,\dots,N-1                                   \\
		              & x_N                    \in \mathcal{X}_f \\
		              & x_0                    = x(0)
	\end{aligned}$$



\subsubsection{Linear Quadratic Optimal Control}

In this section,
only \textbf{linear} discrete-time time-invariant systems

$$x(k + 1) = Ax(k) + Bu(k)$$

and quadratic cost functions

$$
	J(x(0)) :=  x_N^\top P x_N +
	\sum_{i=0}^{N-1}(x_i^\top Q x_i + u_i^\top R u_i)
$$

are considered, and we consider only the problem of
regulating the state to the origin,
\textbf{without state or input constraints}.

The two most common solution approaches will be described here

1. \textbf{Batch Approach}, which yields a
series of \textbf{numerical values} for the input

2. \textbf{Recursive Approach}, which uses Dynamic Programming
to compute control \textbf{policies} or \textbf{laws}, i.e. functions that
describe how the control decisions depend on the system states.

\subsubsection{Unconstrained Finite Horizon Control Problem}
\textbf{Goal} Find a sequence of inputs
$U := [u_0^T,...,u_{N-1}^T]^T$ %FIX: u^T why?
that minimizes the objective function


$$\begin{aligned}
		J^\star(x(0)) :=
		\min_U & x_N^\top P x_N +
		\sum_{i=0}^{N-1}
		x_i^\top Q x_i + u_i^\top R u_i
		\\
		\text{subject to  }
		       & x_{i+1}                = Ax_i+Bu_i
		\quad i  = 0,\dots,N-1                      \\
		       & x_0                    = x(0)
	\end{aligned}$$

$P\succeq0$, with $P=P^T$
terminal weight

$Q\succeq0$, with $Q = Q^T$
state weight

$R\succ0$, with $R = R^T$
input weight

$N$ horizon length

Note that $x(0)$ is the current state, whereas
$x_0,\dots,x_N$ and $x_0,\dots,x_{N-1}$
are optimization variables that are constrained to obey
the system dynamics and the initial condition.

\subsubsection{Batch Approach}
The batch solution explicitly represents
all future states $x_i$ in terms of
initial condition $x_0$ and inputs $u_1,\dots,u_{N-1}$

Starting with $x_0 = x(0)$,
we have $x_1 = Ax(0) + Bu_0$ and
$x_2 = Ax_1 + Bu_1 = A^2 x(0) + ABu_0 + Bu_1$,
by substitution for $x1$, and so on.
Continuing up to $x_N$ we obtain:

$$ \begin{bmatrix}
		x_0    \\
		x_1    \\
		\vdots \\
		\vdots \\
		x_N
	\end{bmatrix}
	=
	\begin{bmatrix}
		\mathbb{I} \\
		A          \\
		\vdots     \\
		\vdots     \\
		A^N
	\end{bmatrix}
	x(0) +
	\begin{bmatrix}
		0        & \cdots & \cdots & 0 \\
		B        & 0      & \cdots & 0 \\
		AB       & B      & \cdots & 0 \\
		\vdots   & \ddots & \ddots & 0 \\
		A^{N-1}B & \cdots & AB     & B
	\end{bmatrix}
	\begin{bmatrix}
		u_0    \\
		u_1    \\
		\vdots \\
		\vdots \\
		u_{N-1}
	\end{bmatrix} $$

The equation above can be represented as

$X := S^x x(0) + S^u U$

\textbf{Cost}

$\overline{Q} := \mathop{\mathrm{blockdiag}}(Q,\dots, Q,P)$

$\overline{R} := \mathop{\mathrm{blockdiag}}(R,\dots, R)$

%NOTE Derivation batch

\textbf{Summary}

The Batch Approach expresses the cost function in terms of
the initial state $x(0)$ and input sequence $U$
by eliminating the states $x_i$.

Because the cost $J(x(0), U)$ is a
positive definite quadratic function of $U$,
its minimizer $U^\star$ is unique and can be found
by setting $\nabla_U J(x(0), U) = 0$

This gives the optimal input sequence $U^\star$
as a linear function of the initial state $x(0)$.

\textbf{Optimal Input}
\[
	U^\star(x(0)) =
	- \bigl(
	\underbrace{
			(\mathcal{S}^u)^\top \overline{Q} \mathcal{S}^u + \overline{R}
		}_{H\text{(Hessian)}^{-1}}
	\bigr)
	\underbrace{
		(\mathcal{S}^u)^\top \overline{Q}\mathcal{S}^x
	}_ {F^\top}
	x(0)
\]

\textbf{Optimal Cost}

\scalebox{0.97}[1]{
	$\scriptstyle
		J^\star(x(0)) = x(0)^\top (
		\mathcal{S}_x^\top \overline{Q} \mathcal{S}_x
		- \mathcal{S}_x^\top \overline{Q} \mathcal{S}_u
		(\mathcal{S}_u^\top \overline{Q} \mathcal{S}_u
		+ \overline{R})^{-1}
		\mathcal{S}_u^\top \overline{Q} \mathcal{S}_x
		)x(0)$
}

\textbf{Note} If there are state or input constraints,
solving this problem by matrix inversion
is not guaranteed to result in a feasible input sequence

\subsubsection{Recursive Approach}

Alternatively, we can use dynamic programming
to solve the same problem in a recursive manner.

Define the $j$-step optimal cost-to-go as the
optimal cost attainable for the step j problem:

\[\begin{aligned}
		J_j^\star(x(j)) :=
		\min_{U_{j\to N}} & x_N^\top P x_N \!+
		\sum_{i=j}^{N-1}x_i^\top Q x_i + u_i^\top R u_i
		\\
		\text{subject to  }
		x_{i+1}           & = Ax_i+Bu_i
		\quad i  = 0,\dots,N-1
		\\
		x_j               & = x(j)
	\end{aligned}\]

This is the minimum cost attainable for the remainder
of the horizon after step $j$

Start at time $N-1$

% NOTE: Steps N- {1,2,...}

Iterate backwards

\textbf{Optimal Control Policy}
\[
	u_i^\star =
	-(B^\top P_{i+1}B + R)^{-1}
	B^\top P_{i+1} A \cdot x(i)
	:= F_i x_i
\]

\textbf{Optimal Cost-To-Go}

\[
	J_i^\star(x_i) = x_i^\top P_i x_i
\]


\textbf{Riccati Difference Equation (RDE)}

\[
	\scriptstyle
	P_i = A^\top P_{i+1} A
	+ Q - A^\top P_{i+1} B
	(B^\top P_{i+1} B + R)^{-1}
	B^\top P_{i+1} A
\]

Find any $P_i$ by recursive evaluation from
$P_N = P$ (the given terminal weight)

Evaluating down to $P_0$, we obtain the $N$-step cost-to-go
\[
	J^\star(x(0)) = J_0^\star(x(0))=x(0)^\top P_0 x(0)
\]

The recursive solution method used from here relies on
Bellman’s \textbf{Principle of Optimality}

For any solution for steps $0$ to $N$ to be optimal,
any solution for steps $j$ to $N$ with $j\ge0$,
taken from the $0$ to $N$ solution,
must itself be optimal for the $j$-to-$N$ problem

Therefore we have, for any $j = 0,\dots,N$
\[\begin{aligned}
		J_j^\star & (x_j) = \min_{u_j}
		I(x_i,u_i) +J_{j+1}^\star(x_{j+1})
		\\
		          & \text{subject to  }
		x_{j+1}     = Ax_j+Bu_j
	\end{aligned} \]

\textbf{Interpretation}

Suppose that the fastest route from Los Angeles to Boston
passes through Chicago. Then the principle of optimality
formalizes the obvious fact that the Chicago to Boston
portion of the route is also the fastest route for a trip
that starts from Chicago and ends in Boston.

\subsubsection{Comparison of Batch and Recursive Approaches}

- Fundamental difference:
Batch optimization returns a sequence $U^\star(x(0))$
of \textbf{numeric values} depending only on the initial state x(0),
while dynamic programming yields \textbf{feedback policies}
$u_i^\star = F_i x_i, i = 0,\dots,N-1$ depending on each $x_i$.

- If the state evolves exactly as modelled,
then the sequences of control actions obtained from
the two approaches are identical.

- The recursive solution should be more robust to disturbances and model
errors, because if the future states later deviate from their predicted
values, the exact optimal input can still be computed.

- The Recursive Approach is computationally more attractive because it
breaks the problem down into single-step problems. For large horizon
length, the Hessian H in the Batch Approach, which must be inverted,
becomes very large.

- Without any modification, both solution methods will break down when
inequality constraints on $x_i$ or $u_i$ are added.

- The Batch Approach is far easier to adapt than the Recursive Approach
when constraints are present: just perform a constrained minimization for
the current state.

\subsection{Receding Horizon}

Receding horizon strategy introduces feedback.

%NOTE: Image  Feedback 02.26

For unconstrained systems, this is a \textbf{constant linear controller}
However, can extend this concept to much more complex systems (MPC)


\subsection{Infinite Horizon LQR}

\subsubsection{Infinite Horizon Control Problem}

\subsubsection{Stability of Infinite Horizon LQR}

%NOTE: LQR detailed

\subsubsection{Choices of Terminal Weight P in Finite Horizon Control}

%TODO: P finite Horizon

\subsubsection{Choices of Terminal Weight P in Finite Horizon Control}

\textbf{Goal} Control law to minimize relative
\textit{energy} of input and state/output

\textbf{Why?}

- Easy to describe objective / tune controller

- Simple to compute and implement

- Proven and effective

\textbf{Why infinite-horizon?}

- Stable

- Optimal solution (doesn’t usually matter)

In MPC we normally cannot have an infinite horizon because
it results in an infinite number of optimization variables.

Use \textit{tricks} to \textit{simulate} quasi-infinite horizon.

\section{Optimization}

\subsection{Learning Objectives}

- Learn to ‘read’ and define optimization problems

- Understand property of convexity of sets and functions

- Understand benefit of convex optimization problems

- Learn and contrast properties of LPs and QPs

- Pose the dual problem to a given primal optimization problem

- Test optimality of a primal and dual solution by means of KKT conditions

- Understand meaning of dual solution for the cost function

\subsection{Main Concepts}

A mathematical optimization problem is generally formulated as:

\begin{equation}
	\begin{aligned}
		\min_{x\in\operatorname{dom}(f)}          & f(x)                      \\
		\text{s.t. }                       g_i(x) & \le 0,\quad i = 1,\dots,m \\
		h_i(x)                                    & = 0,\quad i = 1,\dots,p   \\
	\end{aligned}
	\label{eq:optimization}
\end{equation}

- $x = (x_1,...,x_n) \in \mathbb{R}^{n}$ decision variable

- $f: \operatorname{dom}(f)\to\mathbb{R}$ objectivce function

- $\mathcal{X} = \{x \in \mathbb{R}^{n}: g(\xi)\le0,\ h(\xi)=0\}$ fesabile set


\subsection{Convex Sets}

\begin{definition}[Convex Set]
	A set $\mathcal{C}$ is convex if and only if
	$$\theta x + (1-\theta)y \in \mathcal{C}
		\forall\ x,y \in \mathcal{C},\quad
		\forall\ \theta \in [0,1]$$
\end{definition}

(hyperplane $\parallel$ half-space)
$\{x \in \mathbb{R}^n \mid a\T x(=\parallel\le)b\}$

polyhedra $\{x\in\mathbb{R}^n\mid A^{q\times n}x\preceq b^{q\times1},C^{r\times n}x=d^{r\times1}\}$

polytope



\textbf{Intersection}
$\mathcal{C}_1, \mathcal{C}_2$ cv
$\Rightarrow \mathcal{C}_1 \cap \mathcal{C}_2$ convex \textbf{(cv)}

\textbf{Image under affine map}
$\mathcal{C} \subseteq  \mathbb{R}^{n}$ cv
$\Rightarrow \{Ax+b \mid x \in \mathcal{C} \}$ cv

\textbf{Inverse IoaM}
$\mathcal{C} \subseteq  \mathbb{R}^{m}$ cv
$\Rightarrow \{x\in\mathbb{R}^{n} \mid  Ax+b\in\mathcal{C}\}$ cv

\subsection{Convex Functions}


\textbf{Check Convexity} $f$ is convex if it is
composition of simple convex function
with convexity preserving operations
or if


$f: \mathbb{R}^n \rightarrow \mathbb{R}$ twice differentiable,
$\partial^2f/\partial x^2 \succeq 0\ \forall\ x \in \mathbb{R}^{n}$

$g: \mathbb{R} \rightarrow \mathbb{R}$ with $g(t)=f(x+tv)$
convex in $t\ \forall\ x,v \in \mathbb{R}^{n}$
$\rightarrow f$ convex (restriction to a line)



\subsubsection{Operations that preserve convexity (functions)}

- the point wise maximum of convex functions is convex

- the sum of convex functions is convex

- $f(Ax+b)$ is convex if $f$ is convex

\subsection{Convex Optimization Problems}

%NOTE:Definition cvOP

Optimal value $f^\star =
	\operatorname{inf}\{f(x)\mid
	g_i(x)\le0,h_j=0 \}$
$f^\star=+\infty$ OP is infeasible,
$f^\star=-\infty$ OP is unbound below

%NOTE: Local and Global Optimality

%NOTE: Equivalent  OP, Slack

\subsubsection{Linear Programming}
$ \operatorname{minimize} c\T x
	\text{ s.t. } Ax-b \ge 0,\ x\ge0$

Step 1:
$\mathcal{L}(x,\lambda_1,\lambda_2) =
	c\T x-\lambda_1\T (Ax-b) -\lambda_2\T x,\ \lambda_i \ge0$

Step 2:
$\underset{x \in \mathcal{\mathbb{R}}^n}{\operatorname{inf}}
	\mathcal{L}=\lambda_1\T b$
, if $c-A\T\lambda_1-\lambda_2=0$, else $-\infty$

Step 3: Dual,
maximize $b\T\lambda$
s.t.
$c-A\T\lambda\ge0,\lambda\ge0$
(again LP)

\subsubsection{Quadratic Programming}

\subsection{Optimality Conditions}

\begin{equation}
	\text{We consider}\
	f^\star = \inf_{x\in\mathcal{\mathbb{R}}^n}f(x)
	\;\text{ s.t. }\ g(x)\le0,h(x)=0
	\label{eq:dual}
\end{equation}
%NOTE: Definitions lambda...
$$\begin{aligned}
		                           & \textbf{Lagrange }                                              &
		\mathcal{L}(x,\lambda,\nu) & = f(x) + \lambda\T g(x)+\nu\T h(x)
		\\
		                           & \textbf{Dual Function }                                         &
		d(\lambda,\nu)             & = \inf_{x \in \mathcal{\mathbb{R}}^n}\mathcal{L}(x,\lambda,\nu)
	\end{aligned}$$

%NOTE: Primal Dual (P) (D)

%NOTE: Example LP

%NOTE: Example QP

\subsubsection{Weak and Strong Duality}

\begin{proposition}[Weak Duality]
	$d(\lambda,\nu)\le f^\star,\forall\lambda\ge0,\nu\in\mathbb{R}^{h}$
\end{proposition}

\begin{definition}[Constraint qualification]
	$\mathcal{C}$ convex, \textbf{Slaters Condition} holds if
	$\exists\ \hat{x} \in \mathbb{R}^{n}$ s.t. $h(\hat{x})=0$ and $g(\hat{x})<0$
\end{definition}

\begin{proposition}[Strong Duality]
	If Slater's condition holds
	and (\ref{eq:dual}) is convex
	$\Rightarrow$
	$\exists \lambda \ge 0, \nu \in \mathbb{R}^{n_h}$ s.t. $d(\lambda,\nu)=f^\star$
\end{proposition}


\subsubsection{KKT (Karush-Kuhn-Tucker) Conditions}

\begin{theorem}[KKT Conditions]
	Slater's condition holds
	and (\ref{eq:dual}) is convex
	$\rightarrow$
	$x^\star \in \mathbb{R}^{n}$ is a minimizer of the primal (\ref{eq:dual})
	and $(\lambda^\star \ge 0,\nu^\star) \in \mathbb{R}^{n_g}\times\mathbb{R}^{n_h}$ is a maximizer of the dual
	$\Leftrightarrow$
	$$\begin{aligned}
			 & \nabla_x\mathcal{L}(x^\star,\lambda^\star,\nu^\star)=0
			 & \text{KKT-1 }
			 & \text{(Stationary Lagrangian)}
			\\
			 & g(x^\star)\le0, h(x^\star)=0
			 & \text{KKT-2 }
			 & \text{(primal feasibility)}
			\\
			 & \lambda^\star\ge0, \nu^\star \in \mathbb{R}^{n_h}
			 & \text{KKT-3 }
			 & \text{(dual feasibility)}
			\\
			 & \lambda^{\star T} g(x^\star)=0=\nu^{\star T} h(x^\star)
			 & \text{KKT-4 }
			 & \text{(compementary slackness)}
			\\
		\end{aligned}$$
	In addition we have:
	$\sup_{\lambda\ge0,\nu\in\mathbb{R}^{n_h}}q(\lambda,\nu)=\inf_{x\in\mathcal{C}}f(x)$
\end{theorem}

\textbf{Remark} Without Slater,
KKT1-4 still implies $x^\star$ minimizes (\ref{eq:dual})
and $\lambda,\nu$ maximizes dual,
but the converse is no longer true.
There can be primal-minimizer/dual-maximizer not satisfy KKT.

%NOTE Example KKT QP

%NOTE: Sensitivity Analysis

\subsection{Summary}

- Convex optimization problem:

- Convex cost function

- Convex inequality constraints

- Affine equality constraints

- Benefit of convex problems: Local = Global optimality

- Only need to find one minimum, it is the global minimum!

- For convex optimization problem: If slater condition holds,
x ∗ optimal iff ∃(λ∗ , ν ∗ ) satisfying KKT conditions
%NOTE: symbols
- Convex optimization problems can be solved efficiently

- Many problems can be written as convex opt. problems (with some effort)

Note: Duality and optimality conditions similarly
extend to Convex Cone Programs

\subsubsection{Why did we consider the dual problem?}

- The dual problem is convex, even if the primal is not
%NOTE: symbols
–> can be ‘easier’ to solve than primal

- The dual problem provides a
lower bound for the primal problem: d ∗ ≤ p∗
(and d(λ, ν) ≤ p(x) for all feasible x, λ, ν)

(provides suboptimality bound)

- The dual provides a certificate of optimality
via the KKT conditions for convex problems

- KKT conditions lead to efficient optimization algorithms

- Lagrange multipliers provide information about active constraints
at the optimal solution: if λ∗i > 0, then gi (x ∗ ) = 0

- Lagrange multipliers provide information about sensitivity of optima

