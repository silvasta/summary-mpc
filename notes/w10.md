# Stochastic Learning based MPC

<!-- TODO: formulation MPC -->

## Cautious MPC using GP

### State and Uncertainty Propagation

- Approximation as Normal Distribution

### GP prediction from Uncertainty inputs

- Mean equivalent Approximation
- Taylor-expansion-based Propagation
- Exact Moment Matching (RBF kernels)

<!-- HACK: Table L9.19 -->

Table, Summary \[4\] with a lot of references

### Example PRS for Gaussian

- ellipse

### Cost function under GP

### Cautious MPC using GP formulation

<!-- TODO: formulation MPC -->

## Computational Aspects of Cautious GP MPC

### 1. GP inference

- scales with collecting data

#### use subset of dada (SoD)

- age of data
- informativeness
- locality
- combination of them
<!-- NOTE: chaos -->
#### reduced number of inducing inputs

#### sparse spectrum GP approximation

### 2. Optimization

- increased burden due to additional states

#### Zero-Order Optimization

- SQP
- rewrite QP subproblem
- approximate Jacobian = 0
  - decouples state and covariance dynamics

<!-- TODO: algorithm -->

<!-- WARN: toolbox L4acados -->

- solver, warm start strategy

### 3. Online Learning

#### Finite Basis Function Approximation

- BLR updates, variance smaller, more certain, less change in mean, but problem with changes

form to dynamic, theta as state

- same form as kalman filter

**advantages** no data storage, efficient computation but needs parametrized model with linear equations

#### inducing inputs and seperable kernels

## Robust Guarantees for Stochastic Learning

### assumptions

- R-sub-Gaussion
- joint-in-time chance constraints

### Theorem

<!-- TODO: Theorem -->

- sketch of proof
- easy for p

How to get W?

#### BLR

- easy finding Theta for confident region

#### GP

Distribution of functions

- draw function samples
- they follow joint Gaussian Distribution
- Find bound for GP

Very long derivation...

## Summary Model Learning in MPC
